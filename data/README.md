ðŸ“˜ *This README defines every data stage in the MLOps lifecycle â€” enabling full traceability from raw ingestion to model monitoring.*

# ðŸ“‚ data/

Central repository for all datasets used throughout the ML pipeline.  
Each subfolder represents a different stage of data maturity â€” from raw uploads to monitored model outputs.

```
data/
â”œâ”€ raw/               # untouched incoming data
â”œâ”€ interim/           # cleaned or partially processed
â”œâ”€ processed/         # ready for model training
â”œâ”€ features/          # extracted feature tables
â”œâ”€ validation/        # schema or test data samples
â”œâ”€ predictions/       # model outputs for QA or audit
â””â”€ monitoring/        # drift metrics, logs, performance snapshots
```
---

______________________________________________________________________

SUBFOLDERS

## 1ï¸âƒ£ **raw/** â€“ Incoming Data

Direct dumps from upstream systems or user uploads.  
May include duplicates, missing values, or mixed formats.
Starting point for data processing and ML pipeline.

### ðŸ”¸ Triggered by
ETL, cron job, or API feed
### ðŸ”¸ Triggers
`src/data_prep.py`

### ðŸ”¸ Module
*(none)*
### ðŸ”¸ Command
```bash
aws s3 cp ./new_customer_data.csv s3://mybucket/data/raw/   # AWS example
```

### ðŸ”¸ Example files uploaded
```
data/raw/customers.csv
data/raw/transactions.json
```

---

## 2ï¸âƒ£ **interim/** â€“ Cleaned, Still Volatile

After initial cleaning but before feature engineering.  
Used for quick inspections or debugging preprocessing steps.
Works in memory Pandas Dataframe.
Creates only requested/specified data by pandas module to create new .parquet files

### ðŸ”¸ Triggered by
`src/data_prep.py`
### ðŸ”¸ Triggers
`src/train.py`

### ðŸ”¸ Module
```python
df = pd.read_csv("data/raw/customers.csv")  # read raw
df = df.dropna(subset=["email", "age"]) # clean in memory
df.to_parquet("data/interim/cleaned_customers.parquet") # write clean version
```
### ðŸ”¸ Command
*(none â€” runs automatically in pipeline)*

### ðŸ”¸ Example files created
```
data/interim/cleaned_customers.parquet
data/interim/filtered_sales.csv
```

---

## 3ï¸âƒ£ **processed/** â€“ Final Training Data

Fully transformed datasets, ready for training. Contains the datasets directly used for model training and evaluation.
Used by training and experimentation pipelines.

### ðŸ”¸ Triggered by
`data/interim/cleaned_customers.parquet`
### ðŸ”¸ Triggers
`src/train.py` (model training)

### ðŸ”¸ Module
```python
train_df = pd.read_parquet("data/interim/cleaned_customers.parquet") # Read interim
```

### ðŸ”¸ Example files created
```
data/processed/train.parquet
data/processed/validation.parquet
```

---

## 4ï¸âƒ£ **features/** â€“ Engineered Feature Tables

Optional, used when applying a Feature Store pattern.  
Contains reusable engineered features across models.
Central repository of computed features for training/inference parity.

### ðŸ”¸ Triggered by
`src/train.py` or `src/feature_engineering.py` (if implemented)
### ðŸ”¸ Triggers
Model training and inference that consume shared features MLflow, another pandas script or SageMaker Feature Store

### ðŸ”¸ Module
```python
df = create_feature_table(train_df)
df.to_parquet("data/features/customer_features_v3.parquet")
```

### ðŸ”¸ Example files created
```
data/features/customer_features_v3.parquet
```

---

## 5ï¸âƒ£ **validation/** â€“ Holdout or QA Data

Used to verify pipeline outputs or validate retraining consistency.  
Includes golden datasets for regression and schema testing.
Ensures data quality, schema consistency, and reproducibility.

### ðŸ”¸ Triggered by
CI/CD validation or schema check (e.g., `great_expectations`, `pytest`)
### ðŸ”¸ Triggers
Retraining approval or QA validation jobs, schema checks, or unit tests

### ðŸ”¸ Example files created
```
data/validation/golden_test_set.csv
data/validation/schema_test_sample.json
```

---

## 6ï¸âƒ£ **predictions/** â€“ Model Outputs (QA or Audit)

Stores generated predictions from deployed models for manual review, audit, or monitoring.
Provides traceability for inference results; supports model audits and post-deployment checks.

### ðŸ”¸ Triggered by
`src/serve_app.py` or batch inference scripts
### ðŸ”¸ Triggers
QA or audit pipelines, QA dashboards, monitoring jobs, or compliance tools

### ðŸ”¸ Example files created
```
data/predictions/model_v5_outputs.csv
```

---

## 7ï¸âƒ£ **monitoring/** â€“ Drift Metrics, Logs, Performance Snapshots

Contains logs and metrics for data drift, performance, and system health.
Tracks drift and performance trends over time; ensures deployed models stay reliable and compliant.

### ðŸ”¸ Triggered by
`src/drift_check.py` or CI/CD scheduled monitor
### ðŸ”¸ Triggers
Retraining pipeline or alert notifications. pandas (metrics parsing), dashboards, alert systems

### ðŸ”¸ Module
```python
if drift_score > 0.05:
    trigger_retrain_pipeline()
```

### ðŸ”¸ Example files created
```
data/monitoring/input_stats_2025_10_21.json
data/monitoring/drift_summary.csv
```

---

### âœ… **Summary**

| Stage | Input From | Created By | Ingested By | Triggers |
|--------|-------------|-------------|--------------|-----------|
| `raw/` | Upstream | External source | `data_prep.py` | Data cleaning |
| `interim/` | `data_prep.py` | `data_prep.py` | pandas memory | Training step |
| `processed/` | `data_prep.py` | CI/CD | `train.py` | Training & registry |
| `features/` | Training data | Feature job | Model pipeline | Inference |
| `validation/` | CI/CD | Validator | QA tools | Retraining |
| `predictions/` | Inference jobs | Model API | QA dashboards | Monitoring |
| `monitoring/` | Drift monitor | `drift_check.py` | Dashboards | Retraining trigger |

---

Stage	Format Used	Why
data/raw/	.csv, .json, .xlsx	Human-readable / manual upload
data/interim/	.parquet	Faster intermediate read/write
data/processed/	.parquet	Consistent model input format
data/monitoring/	.csv, .json	Easy for metrics dashboard

__________________________________________________________________________________________

Example: Converting CSV â†’ Parquet with data_prep.py (pandas)
import pandas as pd
# Load raw CSV
df = pd.read_csv("data/raw/customers.csv")
# Clean and filter
df = df.dropna(subset=["email", "age"])
# Save as Parquet
df.to_parquet("data/interim/cleaned_customers.parquet", index=False)

.parquet file is a binary, columnar data format designed for big data and ML pipelines.
Itâ€™s used instead of .csv because itâ€™s:
Much faster to read/write
Compressed (saves cloud storage)
Column-oriented (great for analytics and training data)
Schema-aware (preserves data types)
Parquet stores each column separately (columnar layout).
This allows efficient reads for selected columns only: 
pd.read_parquet("cleaned_customers.parquet", columns=["age", "region"])
tâ€™s the standard format used by tools like Spark, Arrow, Athena, BigQuery, SageMaker, and Azure ML.

Author: David Santana Rivera
Last updated: 10/10/2025